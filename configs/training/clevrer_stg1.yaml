# Global Seed
seed: 0

# Datamodule Config
data_module:
  _target_: wiskers.datasets.clevrer.data_module.ClevrerImage
  data_dir: "./datasets"
  batch_size: 4
  num_workers: 2
  preprocessing:
    _target_: wiskers.datasets.clevrer.data_module.PreprocessingConfig
    chunk_size: 16
    stride: 16
    resize: [160, 240]
    limit: null
    # Optional: enable to upload processed CLEVRER assets to Google Drive
    gdrive_upload:
      enabled: false
      folder_id: null
      credentials_path: null
      archive_name: clevrer_processed.zip
  transform:
    _target_: wiskers.datasets.clevrer.data_module.TransformConfig
    image_size: [64,64] # squared image required to work properly (FIXME)

# Logger Configs
tensor_board_logger:
  save_dir: "./experiments"
  name: "clevrer_stg1"
  version: null

# Export Model
export_onnx: False
export_safetensors: False

# Module Config (model, optimizer and scheduler)
module:
  _target_: wiskers.modules.world_model_module.WorldModelModule
  _recursive_: false  # keep nested configs (e.g., lr_scheduler) as dicts
  # model
  in_channels: 3
  stem_channels: 8
  out_channels: 3
  num_heads: 0 # A value of 0 implies scaled dot-product attention instead of multihead attention.
  block_channels: [16, 32, 64, 128, 256]
  block_attentions: [False, False, False, False, False]
  image_size: ${data_module.transform.image_size}  # Use a reference to data_module.image_size
  activation: "torch.nn.ReLU"
  losses:
    reconstruction: "wiskers.common.losses.MixedL1L2Loss"
    vq_weight: 1.0
    reconstruction_weight: 1.0
    ssim_weight: 0.3
  # codebook
  num_codes: 128
  beta: 0.1
  use_ema: True
  decay: 0.99
  eps: 1e-5
  # optimizer
  optimizer:
    _target_: torch.optim.Adam
    lr: 3e-4
  # lr scheduler
  lr_scheduler:
    _target_: torch.optim.lr_scheduler.StepLR
    step_size: 150
    gamma: 0.5

# Training Configs
earlystopping: False

# lightning.Trainer Config
trainer:
  # Debugging options
  # fast_dev_run: 1        # runs 1 train/val/test batch then quits
  overfit_batches: 1     # force train/val on the same batch every epoch

  # Batch limits
  limit_train_batches: 2   # 1.0 = full dataset, int = exact number of batches
  limit_val_batches: 1
  limit_test_batches: 1

  # Training schedule
  max_epochs: 200
  log_every_n_steps: 10
